{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Code DATA301.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Initial Imports**"
      ],
      "metadata": {
        "id": "C0gFcc3QxNWY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs7qgKnzwif5"
      },
      "outputs": [],
      "source": [
        "#library and code setup\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q pyspark\n",
        "import pyspark, os\n",
        "from pyspark import SparkConf, SparkContext\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n",
        "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-8-openjdk-amd64/\"\n",
        "!pip install gdelt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialise pyspark**"
      ],
      "metadata": {
        "id": "O3ZbgRqjxxi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#start spark local server\n",
        "import sys, os\n",
        "from operator import add\n",
        "import time\n",
        "\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n",
        "\n",
        "import pyspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "#connects our python driver to a local Spark JVM running on the Google Colab server virtual machine\n",
        "try:\n",
        "  conf = SparkConf().setMaster(\"local[*]\").set(\"spark.executor.memory\", \"1g\")\n",
        "  sc = SparkContext(conf = conf)\n",
        "except ValueError:\n",
        "  #it's ok if the server is already started\n",
        "  pass\n",
        "\n",
        "def dbg(x):\n",
        "  \"\"\" A helper function to print debugging information on RDDs \"\"\"\n",
        "  if isinstance(x, pyspark.RDD):\n",
        "    print([(t[0], list(t[1]) if \n",
        "            isinstance(t[1], pyspark.resultiterable.ResultIterable) else t[1])\n",
        "           if isinstance(t, tuple) else t\n",
        "           for t in x.take(100)])\n",
        "  else:\n",
        "    print(x)"
      ],
      "metadata": {
        "id": "45_IeyBlx0F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Data**"
      ],
      "metadata": {
        "id": "w_5QJlW8yH6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from datetime import date, timedelta\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import gdelt\n",
        "import os\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "# set up gdeltpyr for version 2\n",
        "gd = gdelt.gdelt(version=2)\n",
        "\n",
        "# multiprocess the query\n",
        "e = ProcessPoolExecutor()\n",
        "\n",
        "\n",
        "# generic functions to pull and write data to disk based on date\n",
        "def get_filename(x):\n",
        "  date = x.strftime('%Y%m%d')\n",
        "  return \"{}_gdeltdata.csv\".format(date)\n",
        "\n",
        "def intofile(filename):\n",
        "    try:\n",
        "        if not os.path.exists(filename):\n",
        "          date = filename.split(\"_\")[0]\n",
        "          d = gd.Search(date, table='events',coverage=True) #not updata at 15mins\n",
        "          d.to_csv(filename,encoding='utf-8',index=False)\n",
        "    except:\n",
        "        print(\"Error occurred\")\n",
        "\n",
        "# pull the data from gdelt into multi files; this may take a long time\n",
        "# change date range here\n",
        "dates = [get_filename(x) for x in pd.date_range('2022 01 01', '2022 01 31')]\n",
        "\n",
        "\n",
        "results = list(e.map(intofile,dates))\n",
        "\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "data = sqlContext.read.option(\"header\", \"true\").csv(dates)\n",
        "\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=1AlyqTzz8HR1qoeFkaxTUJ_IUU5UUMNVH&export=download'"
      ],
      "metadata": {
        "id": "g9WFwZhWx5xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Code - Outputs top 10 pairs of event types of interest from Actor1CountryCode = 'USA' for highest and lowest values"
      ],
      "metadata": {
        "id": "_QO_EZRLyMQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "def is_good(result):\n",
        "  if result['Actor1CountryCode'] == 'USA':\n",
        "    return True\n",
        "\n",
        "#transforms full months data from csv into rdd with ('USA', '*country*') keys and a list of unique event type codes as values\n",
        "events_by_country = data.rdd.map(lambda row: row).filter(lambda row: is_good(row) == True).map(lambda row: ((row['Actor1CountryCode'], row['Actor2CountryCode']), row['EventCode'])).groupByKey().map(lambda x : (x[0], list(set(x[1]))))\n",
        "\n",
        "#Filters the event types to only pairs containing one of these elements\n",
        "#for example, if we only wanted to test event types of pairs\n",
        "#where one pair is in the provide aid catagory,  we put them into filteredgroup\n",
        "filteredgroup = ['070','071','072','073','074','075']\n",
        "\n",
        "# Maps Rdd into an rdd with the event types and the number of countries it\n",
        "# was associated with, filters to >= 5, and broadcasts as a map\n",
        "eventfreq = events_by_country.flatMap(lambda x: x[1]).map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b).filter(lambda x: x[1] >= 5)\n",
        "eventfreq_b = sc.broadcast(eventfreq.collectAsMap())\n",
        "\n",
        "#eventfreq2 = events_by_country.flatMap(lambda x: x[1]).map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b).filter(lambda x: x[1] >= 5).filter(lambda x: x[0] in filteredgroup)\n",
        "#eventfreq2_b = sc.broadcast(eventfreq2.collectAsMap())\n",
        "\n",
        "def srtTup(a,b):\n",
        "  return (a,b) if a<b else (b,a)\n",
        "\n",
        "def Pairs(listy):\n",
        "  \"\"\"Returns pairs from a given list and checks through filtered list to reduce run time\"\"\"\n",
        "  ret = list()\n",
        "  for a in listy:\n",
        "    for b in listy:\n",
        "      if a in eventfreq_b.value and b in eventfreq_b.value:\n",
        "        if a != b:\n",
        "          if (srtTup(a,b), 1) not in ret:\n",
        "            ret.append((srtTup(a,b), 1))\n",
        "  return ret\n",
        "\n",
        "def Filtered_Pairs(listy):\n",
        "  ret = list()\n",
        "  for a in listy:\n",
        "    for b in listy:\n",
        "      if a in eventfreq_b.value and b in eventfreq_b.value:\n",
        "        if a in eventfreq2_b.value or b in eventfreq2_b.value:\n",
        "          if a != b:\n",
        "            if (srtTup(a,b), 1) not in ret:\n",
        "              ret.append((srtTup(a,b), 1))\n",
        "  return ret\n",
        "\n",
        "# NOTE : Uncomment the pairs with Filtered_Pairs if using Specific event types\n",
        "# Creates Pair and Frequency Tuples using the Pairs function\n",
        "EF2 = events_by_country.map(lambda x: x[1])\n",
        "pairs = EF2.flatMap(lambda x: Pairs(x))\n",
        "#pairs = EF2.flatMap(lambda x: Filtered_Pairs(x))\n",
        "final = pairs.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "lenfile = len(events_by_country.collect())\n",
        "\n",
        "# Calculates Support, Confidence and Interest Scores\n",
        "suppscore = final.map(lambda x: (x[0], x[1] / lenfile))\n",
        "suppscore2 = final.map(lambda x: (tuple(sorted(x[0], reverse = True)), x[1] / lenfile))\n",
        "confscore = suppscore.map(lambda x: (x[0], x[1] / (eventfreq_b.value[x[0][0]] / lenfile) ))\n",
        "confscore2 = suppscore2.map(lambda x: (x[0], x[1] / (eventfreq_b.value[x[0][0]] / lenfile) ))\n",
        "conf_fin = confscore.union(confscore2).sortBy(lambda x: -x[1])\n",
        "\n",
        "interest_fin = conf_fin.map(lambda x: (x[0], x[1] - (eventfreq_b.value[x[0][1]] / lenfile))).sortBy(lambda x: -x[1])\n",
        "interest_fin2 = interest_fin.sortBy(lambda x: x[1])\n",
        "\n",
        "# creates dictionary from event type codes and description pairs\n",
        "transdict = sc.textFile('/content/translation.txt').map(lambda line: line.split('\\t')).collectAsMap()\n",
        "\n",
        "# Gives a list of top 10 event type pairings by interest\n",
        "interesting = sc.parallelize(interest_fin.take(10)).map(lambda data: ((transdict[data[0][0]], transdict[data[0][1]]), data[1]))\n",
        "interesting2 = sc.parallelize(interest_fin2.take(10)).map(lambda data: ((transdict[data[0][0]], transdict[data[0][1]]), data[1]))\n",
        " \n",
        "print(interesting.collect())\n",
        "print(interesting2.collect())\n",
        "\n",
        "time_end = time.time()\n",
        "print(\"elapsed time is %s\" % str(time_end-time_start))\n"
      ],
      "metadata": {
        "id": "B0zDpms2yOHp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}